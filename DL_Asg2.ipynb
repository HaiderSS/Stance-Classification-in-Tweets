{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk                            # Cleaning the data\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,SpatialDropout1D,Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14834533405055759167\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12376883961685384885\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../Pickle_files/train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_pickle('../Pickle_files/yz.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 3)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hillary Clinton                     689\n",
       "Feminist Movement                   664\n",
       "Legalization of Abortion            653\n",
       "Atheism                             513\n",
       "Climate Change is a Real Concern    395\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc =df[df['Target'] == 'Hillary Clinton']\n",
    "fm =df[df['Target'] == 'Feminist Movement']\n",
    "la =df[df['Target'] == 'Legalization of Abortion']\n",
    "at =df[df['Target'] == 'Atheism']\n",
    "cc =df[df['Target'] == 'Climate Change is a Real Concern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_test =tdf[tdf['Target'] == 'Atheism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exalts shall humbled humbles shall exaltedmatt</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prayerbullets remove nehushtan previous move g...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brainman heidtjj benjaminlives sought truth so...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>god utterly powerless human intervention</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>davidcameron miracle multiculturalism miracle ...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>afraid apologise using word god analogy explai...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>soon think wwiii wwwiv begin utedwestand endra...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>humble trust god lean trust confident lord hap...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>newhorizons fly pluto newhorizons plutoflyby p...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>woo god woo department claudia godisloveislove...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet   Target   Stance\n",
       "0       exalts shall humbled humbles shall exaltedmatt  Atheism  AGAINST\n",
       "1    prayerbullets remove nehushtan previous move g...  Atheism  AGAINST\n",
       "2    brainman heidtjj benjaminlives sought truth so...  Atheism  AGAINST\n",
       "3             god utterly powerless human intervention  Atheism  AGAINST\n",
       "4    davidcameron miracle multiculturalism miracle ...  Atheism  AGAINST\n",
       "..                                                 ...      ...      ...\n",
       "215  afraid apologise using word god analogy explai...  Atheism  AGAINST\n",
       "216  soon think wwiii wwwiv begin utedwestand endra...  Atheism    FAVOR\n",
       "217  humble trust god lean trust confident lord hap...  Atheism  AGAINST\n",
       "218  newhorizons fly pluto newhorizons plutoflyby p...  Atheism    FAVOR\n",
       "219  woo god woo department claudia godisloveislove...  Atheism  AGAINST\n",
       "\n",
       "[220 rows x 3 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = at['Tweet']\n",
    "target = at['Stance']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split( train_X, target , test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fm_X = fm['Tweet']\n",
    "# target = fm['Stance']\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split( train_X, target , test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = hc['Tweet']\n",
    "# target = hc['Stance']\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split( train_X, target , test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = hc['Tweet']\n",
    "# target = hc['Stance']\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split( train_X, target , test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# max length of the review\n",
    "\n",
    "r_len=[]\n",
    "for text in hc['Tweet']:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print (MAX_REVIEW_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 1839\n",
    "max_word = 350\n",
    "batch_size = 128\n",
    "epochs = 6\n",
    "num_class = 3\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_feature)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(at_test['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_word)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sequence.pad_sequences(X_test, maxlen=max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410, 350) (103, 350)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 350)\n"
     ]
    }
   ],
   "source": [
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coef(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def get_emb_matrix(EMB_FILE, max_feature,emb_dimension):\n",
    "    # word vectors\n",
    "    emb_index = dict(get_coef(*x.rstrip().rsplit(' ')) for x in open(EMB_FILE, encoding='utf8'))\n",
    "    print('Found %s word vectors.' % len(emb_index))\n",
    "\n",
    "    # embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_feature, len(word_index) + 1)\n",
    "    all_emb = np.stack(emb_index.values()) #for random init\n",
    "    emb_matrix = np.random.normal(all_emb.mean(), all_emb.std(), (num_words, emb_dimension))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_feature:\n",
    "            continue\n",
    "        emb_vector = emb_index.get(word)\n",
    "        if emb_vector is not None:\n",
    "            emb_matrix[i] = emb_vector\n",
    "    max_feature = emb_matrix.shape[0]\n",
    "    return emb_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(1839, 300)\n"
     ]
    }
   ],
   "source": [
    "EMB_FILE = '../Embeddings/glove.6B.300d.txt'\n",
    "\n",
    "emb_dimension = 300 #word vector dim\n",
    "emb_matrix = get_emb_matrix(EMB_FILE,max_feature,emb_dimension)\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "Y_train = le.transform(Y_train)\n",
    "Y_val = le.transform(Y_val)\n",
    "\n",
    "Y_train = to_categorical(Y_train, dtype =\"uint8\") \n",
    "Y_val = to_categorical(Y_val, dtype =\"uint8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = le.transform(at_test['Stance'])\n",
    "label_encode_y_test = Y_test\n",
    "Y_test = to_categorical(Y_test, dtype =\"uint8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839 300 350\n"
     ]
    }
   ],
   "source": [
    "print (max_feature, emb_dimension, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 350, 300)          551700    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 350, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 350, 256)          439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               164352    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,155,735\n",
      "Trainable params: 1,155,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_feature, emb_dimension, input_length=X_train.shape[1],weights=[emb_matrix],trainable=True))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "# model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.9828 - accuracy: 0.5537 - val_loss: 1.0572 - val_accuracy: 0.5243\n",
      "Epoch 2/15\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.8663 - accuracy: 0.6195 - val_loss: 0.9238 - val_accuracy: 0.5340\n",
      "Epoch 3/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.6574 - accuracy: 0.7220 - val_loss: 0.8284 - val_accuracy: 0.6796\n",
      "Epoch 4/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.5246 - accuracy: 0.8049 - val_loss: 0.8000 - val_accuracy: 0.6893\n",
      "Epoch 5/15\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.3239 - accuracy: 0.8707 - val_loss: 0.7070 - val_accuracy: 0.7087\n",
      "Epoch 6/15\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.1833 - accuracy: 0.9561 - val_loss: 0.8018 - val_accuracy: 0.7087\n",
      "Epoch 7/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.1089 - accuracy: 0.9683 - val_loss: 0.9416 - val_accuracy: 0.7184\n",
      "Epoch 8/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.0876 - accuracy: 0.9683 - val_loss: 1.0486 - val_accuracy: 0.6505\n",
      "Epoch 9/15\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.0512 - accuracy: 0.9878 - val_loss: 0.9165 - val_accuracy: 0.7282\n",
      "Epoch 10/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.0489 - accuracy: 0.9805 - val_loss: 1.0231 - val_accuracy: 0.6990\n",
      "Epoch 11/15\n",
      "13/13 [==============================] - 15s 1s/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.9605 - val_accuracy: 0.7379\n",
      "Epoch 12/15\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.0510 - accuracy: 0.9878 - val_loss: 1.1297 - val_accuracy: 0.7087\n",
      "Epoch 13/15\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.0456 - accuracy: 0.9854 - val_loss: 1.0919 - val_accuracy: 0.7476\n",
      "Epoch 14/15\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.0699 - accuracy: 0.9780 - val_loss: 1.0998 - val_accuracy: 0.6893\n",
      "Epoch 15/15\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.0273 - accuracy: 0.9902 - val_loss: 1.0242 - val_accuracy: 0.7087\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=15, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted  = []\n",
    "for i in y_pred:\n",
    "    y_test_predicted.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[132,  10,  18],\n",
       "       [ 17,   6,   9],\n",
       "       [  6,   0,  22]], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(label_encode_y_test.tolist(), y_test_predicted, labels =[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
